{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gamma, cauchy, multivariate_normal, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blr(y, F, mu_0, Sigma_0, a_0, b_0, n_draws=1):\n",
    "    n = y.shape[0]\n",
    "    p = mu_0.shape[0]\n",
    "    mu_post = np.matmul(np.linalg.inv(np.matmul(np.transpose(F), F) + Sigma_0),\n",
    "                        np.matmul(Sigma_0, mu_0) + np.matmul(np.transpose(F), y))\n",
    "    Sigma_post = np.matmul(np.transpose(F), F) + Sigma_0\n",
    "    a_post = a_0 + n / 2\n",
    "    b_post = b_0 + 0.5 * (np.matmul(np.transpose(y), y) + \n",
    "                          np.matmul(np.matmul(np.transpose(mu_0), Sigma_0), mu_0) - \n",
    "                          np.matmul(np.matmul(np.transpose(mu_post), Sigma_post), mu_post))\n",
    "    beta = np.empty([n_draws, p])\n",
    "    sigma2_eps = 1 / np.random.gamma(a_post, 1 / b_post, n_draws)\n",
    "    \n",
    "    for i in range(n_draws):\n",
    "        beta[i, :] = np.random.multivariate_normal(mu_post, sigma2_eps[i] * np.linalg.inv(Sigma_post))\n",
    "    return beta, sigma2_eps\n",
    "\n",
    "\n",
    "def blr_mv(y, F, mu_0, Sigma_0, a_0, b_0):\n",
    "    q = y.shape[1]\n",
    "    r = F.shape[1]\n",
    "    beta_est = np.empty([r, q])\n",
    "    sigma2_eps_est = np.empty(q)\n",
    "    for j in range(q):\n",
    "        results = blr(y[:, j], F, mu_0, Sigma_0, a_0, b_0)\n",
    "        beta_est[:, j] = results[0]\n",
    "        sigma2_eps_est[j] = results[1]\n",
    "    return beta_est, sigma2_eps_est\n",
    "\n",
    "\n",
    "def sample_regression_posterior(Y, F, loading_prior_params=[0, 1], variance_prior_params=[1, 1]):\n",
    "    r = F.shape[1]\n",
    "    mu_0 = np.repeat(loading_prior_params[0], r)  # prior for regression coefficients\n",
    "    Sigma_0 = np.diag(np.repeat(loading_prior_params[1], r)) \n",
    "    loading, variance = blr_mv(Y, F, mu_0, Sigma_0, variance_prior_params[0], variance_prior_params[1])\n",
    "    return loading, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_covariance_blocks(F_covariance_list, loading_matrix, Y_variance):\n",
    "    \"\"\"\n",
    "    Build covariance matrix for long vector of all columns of Y stacked together.\n",
    "    \n",
    "    Args\n",
    "        F_covariance_list: (list) of [t, t] covariance matrices\n",
    "        loading_matrix: (numpy array) [r, q] linear transformation between F and Y\n",
    "        Y_sigma_list: (numpy array) [q] variance parameters for columns of Y\n",
    "    \"\"\"\n",
    "    r = len(F_covariance_list)\n",
    "    t = F_covariance_list[0].shape[0]\n",
    "    q = loading_matrix.shape[1]\n",
    "    block_YY = np.zeros((q * t, q * t))\n",
    "    # covariance for columns of F\n",
    "    block_FF_rows = []\n",
    "    for i in range(r):\n",
    "        current_row = np.zeros((t, r * t))\n",
    "        current_row[:, (i * t):(i * t + t)] = F_covariance_list[i]\n",
    "        block_FF_rows.append(current_row)\n",
    "    block_FF = np.vstack(block_FF_rows)\n",
    "    # covariance between columns of F and columns of Y\n",
    "    block_FY_rows = []\n",
    "    for i in range(r):\n",
    "        current_row = np.zeros((t, q * t))\n",
    "        for j in range(q):\n",
    "            current_row[:, (j * t):(j * t + t)] = loading_matrix[i, j] * F_covariance_list[i]\n",
    "        block_FY_rows.append(current_row)\n",
    "    block_FY = np.vstack(block_FY_rows)\n",
    "    block_YF = np.transpose(block_FY)\n",
    "    # covariance between columns of Y\n",
    "    block_YY_rows = []\n",
    "    for i in range(q):\n",
    "        current_row = np.zeros((t, q * t))\n",
    "        for j in range(q):\n",
    "            for k in range(r):\n",
    "                current_row[:, (j * t):(j * t + t)] += F_covariance_list[k] * loading_matrix[k, i] * loading_matrix[k, j]\n",
    "            if i == j:\n",
    "                current_row[:, (j * t):(j * t + t)] += np.eye(t) * Y_variance[i]  # diagonal variance\n",
    "        block_YY_rows.append(current_row)\n",
    "    block_YY = np.vstack(block_YY_rows)\n",
    "    return block_FF, block_FY, block_YF, block_YY\n",
    "\n",
    "\n",
    "def sample_conditional_F(Y, blocks, debug=False):\n",
    "    \"\"\"\n",
    "    Sample from conditional distribution of F given everything else.\n",
    "    \n",
    "    Args\n",
    "        Y: (numpy array) [t, q] observed multivariate time series\n",
    "        block_FF, block_FY, block_YF, block_YY: (numpy array) blocks in the covariance of joint distribution\n",
    "    \"\"\"\n",
    "    block_FF, block_FY, block_YF, block_YY = blocks\n",
    "    t, q = Y.shape\n",
    "    r = int(block_FF.shape[0] / t)\n",
    "    Y_stack = np.transpose(Y).reshape(t * q)  # stack columns of Y\n",
    "    block_YY_inverse = np.linalg.inv(block_YY)\n",
    "    prod = np.matmul(block_FY, block_YY_inverse)\n",
    "    mu = np.matmul(prod, Y_stack)\n",
    "    covariance = block_FF - np.matmul(prod, block_YF)\n",
    "    F_stack = np.random.multivariate_normal(mu, covariance)\n",
    "    F_sample = np.transpose(F_stack.reshape((r, t)))  # de-stack columns of F\n",
    "    if debug:\n",
    "        return F_sample, covariance\n",
    "    else:\n",
    "        return F_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_gamma_prior(l, a, b):\n",
    "    \"\"\"\n",
    "    Gamma prior on length scale\n",
    "    \"\"\"\n",
    "    return gamma.pdf(l, a=a, scale=1/b)\n",
    "\n",
    "\n",
    "def s_half_cauchy_prior(s, scale):\n",
    "    return 2 * cauchy.pdf(s, loc=0, scale=scale)\n",
    "\n",
    "\n",
    "def kernel_covariance(x, l, s, noise=1e-6):\n",
    "    \"\"\"\n",
    "    Covariance matrix with squared exponential kernel\n",
    "    \"\"\"\n",
    "    t = x.shape[0]\n",
    "    cov_sample = np.zeros((t, t))\n",
    "    for i in range(t):\n",
    "        for j in range(t):\n",
    "            cov_sample[i, j] = s ** 2 * np.exp(-(x[i] - x[j]) ** 2 / (2 * l ** 2))\n",
    "    cov_sample += np.eye(t) * noise  # add noise for numerical stability\n",
    "    return cov_sample\n",
    "\n",
    "\n",
    "def reshape_latent_curves(F, n, t):\n",
    "    \"\"\"\n",
    "    Turn latent factors F of shape [nt, r] into a list of r factors of shape [t, n]\n",
    "    \"\"\"\n",
    "    r = F.shape[1]\n",
    "    F_curves_list = []\n",
    "    for j in range(r):\n",
    "        F_curves = np.zeros((t, n))\n",
    "        for i in range(n):\n",
    "            F_curves[:, i] = F[(i * t):(i * t + t), j]\n",
    "        F_curves_list.append(F_curves)\n",
    "    return F_curves_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accept_reject(accept_prob):\n",
    "    u = np.random.uniform(low=0.0, high=1.0, size=1)\n",
    "    return u < accept_prob\n",
    "\n",
    "\n",
    "def gp_marginal_likelihood(y, x, l, s):\n",
    "    \"\"\"\n",
    "    Marginal likelihood of one Gaussian process (multivariate Normal)\n",
    "    \"\"\"\n",
    "    t = y.shape[0]\n",
    "    mu = np.repeat(0, t)\n",
    "    cov = kernel_covariance(x, l, s)\n",
    "    return multivariate_normal.pdf(y, mu, cov)\n",
    "\n",
    "\n",
    "def propose(current, std):\n",
    "    value = -1\n",
    "    while value < 0:\n",
    "        value = np.random.normal(loc=current, scale=std, size=1)\n",
    "    return value\n",
    "\n",
    "\n",
    "def calculate_p(l, s, Y, x, prior_params):\n",
    "    \"\"\"\n",
    "    Calculate log prior and likelihood of n independent Gaussian processes (Y has shape [t, n])\n",
    "    \"\"\"\n",
    "    a, b, scale = prior_params\n",
    "    prior = l_gamma_prior(l, a, b)  # * s_half_cauchy_prior(s, scale)\n",
    "    loglik = 0.0\n",
    "    for j in range(Y.shape[1]):\n",
    "        loglik += np.log(gp_marginal_likelihood(Y[:, j], x, l, s))  # independent observations\n",
    "    return np.log(prior) + loglik\n",
    "\n",
    "\n",
    "def metropolis_update(l, s, p, Y, x, prior_params, proposal_scales):\n",
    "    l_new = propose(l, proposal_scales[0])\n",
    "    s_new = 1.0\n",
    "\n",
    "    p_new = calculate_p(l_new, s_new, Y, x, prior_params)\n",
    "    \n",
    "    if accept_reject(np.exp(p_new - p)):\n",
    "        return l_new, s_new, p_new\n",
    "    else:\n",
    "        return l, s, p\n",
    "\n",
    "\n",
    "def metropolis_sample(n_iter, Y, x, prior_params, proposal_scales):\n",
    "    # initial length scale proposal is centered at Gamma prior mean\n",
    "    l = propose(prior_params[0] * prior_params[1], proposal_scales[0])\n",
    "    s = 1.0\n",
    "    p = calculate_p(l, s, Y, x, prior_params)\n",
    "    \n",
    "    l_trace = []\n",
    "    s_trace = []\n",
    "    for i in range(n_iter):\n",
    "        l, s, p = metropolis_update(l, s, p, Y, x, prior_params, proposal_scales)\n",
    "        l_trace.append(l)\n",
    "        s_trace.append(s)\n",
    "        \n",
    "    return l_trace, s_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FactorGP:\n",
    "    \"\"\"\n",
    "    Latent factor Gaussian process model for multivariate time series \n",
    "    Data: n epochs, t time points, q dimensional, r latent factors\n",
    "    Parameters: loading matrix (r x q), variance vector (q), length scale (r)\n",
    "    Priors: Conjugate Normal, Inverse-Gamma, and Gamma (needs to be informative)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims, mu_var=[0, 1], inverse_gamma=[1, 1], gamma=[10, 1], F=None):\n",
    "        n, t, q, r = dims\n",
    "        self.dims = dims\n",
    "        self.x = np.linspace(1, t, t)  # time points are indexed by intergers from 1 to t\n",
    "        self.loading_prior_params = mu_var  # prior mean and variance for loading coeffcients\n",
    "        self.variance_prior_params = inverse_gamma  # inverse Gamma prior for variance\n",
    "        self.length_prior_params = gamma  # Gamma prior on length scale\n",
    "        #self.kernel_type = 'default'\n",
    "        self.loading, self.variance, self.theta = self.__initiate_params(dims, mu_var, inverse_gamma, gamma)\n",
    "        self.F = F\n",
    "\n",
    "\n",
    "    def __initiate_params(self, dims, mu_var, inverse_gamma, gamma):\n",
    "        n, t, q, r = dims\n",
    "        loading = np.random.normal(mu_var[0], np.sqrt(mu_var[1]), [r, q])\n",
    "        variance = np.random.normal(0, 0.5, q) ** 2  # ad-hoc\n",
    "        theta = np.repeat(gamma[0] * gamma[1], r)  # set length scale to gamma mean\n",
    "        return loading, variance, theta\n",
    "\n",
    "\n",
    "    def conditional_latent(self, Y):\n",
    "        n, t, q, r = self.dims\n",
    "        covs = []\n",
    "        for l in self.theta:\n",
    "            covs.append(kernel_covariance(self.x, l, 1.0))\n",
    "        blocks = build_covariance_blocks(covs, self.loading, self.variance)\n",
    "        F = np.zeros((n * t, r))\n",
    "        # sample from F conditional distribution for each epoch independently\n",
    "        for i in range(n):\n",
    "            F[(i * t):(i * t + t), :] = sample_conditional_F(Y[(i * t):(i * t + t), :], blocks)\n",
    "        self.F = F\n",
    "    \n",
    "\n",
    "    def predict(self):\n",
    "        return np.matmul(self.F, self.loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs_update_params(Y, model, chain_size=50, proposal_std=0.5):\n",
    "    \"\"\"\n",
    "    Sample from model parameter posterior given Y and F \n",
    "    \n",
    "    Note: the model does not change at all\n",
    "    \"\"\"\n",
    "    n, t, q, r = model.dims\n",
    "    if model.F is None:\n",
    "        model.conditional_latent(Y)\n",
    "    F = model.F  # get current latent and condition on it for sampling\n",
    "    loading, variance = sample_regression_posterior(Y, F, model.loading_prior_params, model.variance_prior_params)\n",
    "    theta = np.zeros(r)\n",
    "    traces = np.zeros((r, chain_size))\n",
    "    F_curves_list = reshape_latent_curves(F, n, t)\n",
    "    for i, F_curves in enumerate(F_curves_list):\n",
    "        l_trace, s_trace = metropolis_sample(chain_size, F_curves, model.x, \n",
    "                                             prior_params=[model.length_prior_params[0], model.length_prior_params[1], 1.0], \n",
    "                                             proposal_scales=[proposal_std, proposal_std])\n",
    "        theta[i] = l_trace[-1]\n",
    "        traces[i, :] = l_trace\n",
    "    return F, loading, variance, theta, traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gibbs(Y, model, n_steps, chain_size, proposal_std, verbose=False):\n",
    "    \"\"\"\n",
    "    Run Metropolis with-in Gibbs sampler on latent factor GP model using data Y\n",
    "    \n",
    "    Args\n",
    "        Y: (numpy array) of shape [nt, q] and contains n stacked epochs of [t, q]\n",
    "        model: latent factor GP model\n",
    "        n_steps: (int) number of steps\n",
    "        chain_size: (int) size of Metropolis chain at each iteration\n",
    "        proposal_std: (float) standard deviation of Metropolis proposal distribution\n",
    "        verbose: (bool) whether or not to print out MSE and length scale (for sanity check)\n",
    "    \"\"\"\n",
    "    n, t, q, r = model.dims\n",
    "    \n",
    "    F_sample = np.zeros((n_steps, n * t, r))\n",
    "    loading_sample = np.zeros((n_steps, r, q))\n",
    "    variance_sample = np.zeros((n_steps, q))\n",
    "    theta_sample = np.zeros((n_steps, r))\n",
    "    traces_hist = np.zeros((n_steps, r, chain_size))\n",
    "    mse_history = np.zeros(n_steps)\n",
    "    \n",
    "    for i in tqdm(range(n_steps)):\n",
    "        F, loading, variance, theta, traces = gibbs_update_params(Y, model, chain_size, proposal_std)\n",
    "        # update model parameters and predict\n",
    "        model.loading = loading\n",
    "        model.variance = variance\n",
    "        model.theta = theta\n",
    "        Y_hat = model.predict()\n",
    "        mse = np.mean((Y - Y_hat) ** 2)\n",
    "        if verbose:\n",
    "            print('Current MSE: {}'.format(mse))\n",
    "            print('Current length scale: {}'.format(theta))\n",
    "        # save everything\n",
    "        F_sample[i, :, :] = F\n",
    "        loading_sample[i, :, :] = loading \n",
    "        variance_sample[i, :] = variance\n",
    "        theta_sample[i, :] = theta\n",
    "        traces_hist[i, :, :] = traces\n",
    "        mse_history[i] = mse\n",
    "        # update model latent factors\n",
    "        model.conditional_latent(Y)\n",
    "    return F_sample, loading_sample, variance_sample, theta_sample, traces_hist, mse_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(length_scale, n_time, n_epochs, q, r, std_eps):\n",
    "    \"\"\"\n",
    "    Generate data from an LFGP model with random loading.\n",
    "    \n",
    "    Args:\n",
    "        length_scale: latent factor GP length scale \n",
    "        n_time: length of generated factors\n",
    "        n_epochs: number of observations\n",
    "        q: number of signals\n",
    "        r: number of latent factors\n",
    "        std_eps: standard deviation of idiosyncratic errors\n",
    "    \n",
    "    Returns: \n",
    "        Y: the generated observed data Y, \n",
    "        F_true: the randomly sampled latent factors , \n",
    "        loading: random loading matrix drawn from rq-standard normal.\n",
    "    \"\"\"\n",
    "    gp = GaussianProcessRegressor(kernel=RBF(length_scale=length_scale))\n",
    "\n",
    "    x = np.linspace(1, n_time, n_time)\n",
    "    curves = gp.sample_y(x.reshape((n_time, 1)), n_epochs * r)\n",
    "\n",
    "    F_true = np.zeros((n_epochs * n_time, r))\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(r):\n",
    "            F_true[(i * n_time):(i * n_time + n_time), j] = curves[:, (i * r + j)]\n",
    "    loading = np.random.normal(0, 1, [r, q])\n",
    "    Y = np.matmul(F_true, loading) + np.random.normal(0, std_eps, [n_epochs * n_time, q])\n",
    "    return Y, F_true, loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cache file.\n"
     ]
    }
   ],
   "source": [
    "import os, pickle\n",
    "from functools import wraps\n",
    "\n",
    "### Simulations are computed using run_sim. \n",
    "### For each call of run_sim(), memoize() checks the cache file to see if the simulation \n",
    "### results already exist, and returns them if so.  Else, runs the simulation \n",
    "### for the given settings.\n",
    "###\n",
    "### memoized results are hashed by the full set of arguments of run_sim.\n",
    "\n",
    "def memoize(func):\n",
    "    \"\"\"\n",
    "    Memoizer for run_sim function.\n",
    "    \"\"\"\n",
    "    cache_file = 'simulation_results.pkl'\n",
    "    if os.path.exists(cache_file):\n",
    "        print('Reading cache file.')\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cache = pickle.load(f)\n",
    "    else:\n",
    "        cache = {}\n",
    "        \n",
    "    @wraps(func)\n",
    "    def wrap(*args):\n",
    "        if args not in cache:\n",
    "            print('Running simulation.')\n",
    "            cache[args] = func(*args)\n",
    "            # update the cache file\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(cache, f)\n",
    "        else:\n",
    "            print('Simulation results retrieved from cache.')\n",
    "        return cache[args]\n",
    "    return wrap\n",
    "\n",
    "@memoize\n",
    "def run_sim(seed, length_scale, n_time, n_epochs, q, r, std_eps, n_steps, chain_size, proposal_std):\n",
    "    \"\"\"\n",
    "    Generate data and fit LFGP model.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    Y, F_true, loading = generate_data(length_scale, n_time, n_epochs, q, r, std_eps)\n",
    "\n",
    "    model = FactorGP([n_epochs, n_time, q, r])\n",
    "    model.conditional_latent(Y)\n",
    "\n",
    "    F = model.F\n",
    "    F, loading, variance, theta, traces = gibbs_update_params(Y, model, chain_size, proposal_std)\n",
    "    results = run_gibbs(Y, model, n_steps=n_steps, chain_size=chain_size, proposal_std=proposal_std, verbose=True)\n",
    "    F_sample, loading_sample, variance_sample, theta_sample, traces_hist, mse_history = results\n",
    "\n",
    "    sim_data = {'length_scale' : length_scale, 'n_time' : n_time, 'n_epochs' : n_epochs, \n",
    "                'q' : q, 'r' : r, 'std_eps' : std_eps, \n",
    "                'n_steps' : n_steps, 'chain_size' : chain_size, 'proposal_std' : proposal_std,\n",
    "                'loading' : loading, 'Y' : Y, 'F_true' : F_true, \n",
    "                'F_sample' : F_sample, 'loading_sample' : loading_sample, 'variance_sample' : variance_sample,\n",
    "                'theta_sample' : theta_sample, 'traces_hist' : traces_hist, 'mse_history' : mse_history}\n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simulation Parameters\n",
    "seed = 1\n",
    "length_scale = 10\n",
    "n_time = 50\n",
    "n_epochs = 2\n",
    "q = 3\n",
    "r = 2\n",
    "std_eps = 0.5\n",
    "n_steps = 100\n",
    "chain_size = 50\n",
    "proposal_std = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation results retrieved from cache.\n"
     ]
    }
   ],
   "source": [
    "sim_data = run_sim(seed, length_scale, n_time, n_epochs, q, r, std_eps, n_steps, chain_size, proposal_std)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
